{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source:\n",
    "\n",
    "-  http://archive.ics.uci.edu/ml/datasets/online+retail\n",
    "-  http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_excel('data/Online Retail.xlsx')\n",
    "\n",
    "with open('data/df_retail.bin', 'wb') as f_out:\n",
    "    pickle.dump(df, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reuse the pickled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/df_retail.bin', 'rb') as f_in:\n",
    "    df = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some simple data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "df = df[~df.invoiceno.astype('str').str.startswith('C')].reset_index(drop=True)\n",
    "df.customerid = df.customerid.fillna(-1).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "* StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "* Description: Product (item) name. Nominal.\n",
    "* Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "* InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "* UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "* CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "* Country: Country name. Nominal, the name of the country where each customer resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stockcode_values = df.stockcode.astype('str')\n",
    "\n",
    "stockcodes = sorted(set(stockcode_values))\n",
    "stockcodes = {c: i for (i, c) in enumerate(stockcodes)}\n",
    "\n",
    "df.stockcode = stockcode_values.map(stockcodes).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df[df.invoicedate < '2011-10-09']\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df[(df.invoicedate >= '2011-10-09') & \n",
    "            (df.invoicedate <= '2011-11-09') ]\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df[df.invoicedate >= '2011-11-09']\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3527, 3506, 1347, 2730,  180])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = df_train.stockcode.value_counts().head(5).index.values\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_indptr(df):\n",
    "    indptr, = np.where(df.invoiceno != df.invoiceno.shift())\n",
    "    indptr = np.append(indptr, len(df)).astype('int32')\n",
    "    return indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_indptr = group_indptr(df_val)\n",
    "num_groups = len(val_indptr) - 1\n",
    "baseline = np.tile(top, num_groups).reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def precision(group_indptr, true_items, predicted_items):\n",
    "    tp = 0\n",
    "\n",
    "    n, m = predicted_items.shape\n",
    "\n",
    "    for i in range(n):\n",
    "        group_start = group_indptr[i]\n",
    "        group_end = group_indptr[i + 1]\n",
    "        group_true_items = true_items[group_start:group_end]\n",
    "\n",
    "        for item in group_true_items:\n",
    "            for j in range(m):\n",
    "                if item == predicted_items[i, j]:\n",
    "                    tp = tp + 1\n",
    "                    continue\n",
    "\n",
    "    return tp / (n * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0642299794661191"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_items = df_val.stockcode.values\n",
    "precision(val_indptr, val_items, baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark #2: ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_user = df_train[df_train.customerid != -1].reset_index(drop=True)\n",
    "\n",
    "customers = sorted(set(df_train_user.customerid))\n",
    "customers = {c: i for (i, c) in enumerate(customers)}\n",
    "\n",
    "df_train_user.customerid = df_train_user.customerid.map(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uid = df_train_user.customerid.values.astype('int32')\n",
    "iid = df_train_user.stockcode.values.astype('int32')\n",
    "ones = np.ones_like(uid, dtype='uint8')\n",
    "\n",
    "X_train = sp.csr_matrix((ones, (uid, iid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_val.customerid = df_val.customerid.apply(lambda c: customers.get(c, -1))\n",
    "\n",
    "uid_val = df_val.drop_duplicates(subset='invoiceno').customerid.values\n",
    "known_mask = uid_val != -1\n",
    "uid_val = uid_val[known_mask] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13848049281314168"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_baseline = baseline.copy()\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "item_user = X_train.T.tocsr()\n",
    "als = AlternatingLeastSquares(factors=128, regularization=0.000001, )\n",
    "als.fit(item_user)\n",
    "\n",
    "als_U = als.user_factors\n",
    "als_I = als.item_factors\n",
    "\n",
    "pred_all = als_U[uid_val].dot(als_I.T)\n",
    "top_val = (-pred_all).argsort(axis=1)[:, :5]\n",
    "imp_baseline[known_mask] = top_val\n",
    "\n",
    "precision(val_indptr, val_items, imp_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's do some tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_batches(seq, step):\n",
    "    n = len(seq)\n",
    "    res = []\n",
    "    for i in range(0, n, step):\n",
    "        res.append(seq[i:i+step])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_users = uid.max() + 1\n",
    "num_items = iid.max() + 1\n",
    "\n",
    "num_factors = 128\n",
    "lambda_user  = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "K = 5\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed(inputs, size, dim, name=None):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup\n",
    "\n",
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_user = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_item = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "    user_factors = embed(place_user, num_users, num_factors, \"user_factors\")\n",
    "    user_bias = embed(place_user, num_users, 1, \"user_bias\")\n",
    "    user_bias = tf.reshape(user_bias, [-1, 1])\n",
    "\n",
    "    item_factors = embed(place_item, num_items, num_factors, \"item_factors\")\n",
    "    item_bias = embed(place_item, num_items, 1, \"item_bias\")\n",
    "    item_bias = tf.reshape(item_bias, [-1, 1])\n",
    "\n",
    "    global_bias = tf.Variable(0.0, name='global_bias')\n",
    "\n",
    "    pred = tf.reduce_sum(user_factors * item_factors, axis=2)\n",
    "    pred = tf.sigmoid(global_bias + user_bias + item_bias + pred)\n",
    "\n",
    "    reg = lambda_user * tf.reduce_sum(user_factors * user_factors) + \\\n",
    "          lambda_item * tf.reduce_sum(item_factors * item_factors)\n",
    "\n",
    "    loss = tf.losses.log_loss(place_y, pred)\n",
    "    loss_total = loss + reg\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss_total)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variable(graph, session, name):\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    return v\n",
    "\n",
    "def calculate_validation_precision(graph, session, uid):\n",
    "    U = get_variable(graph, session, 'user_factors')\n",
    "    I = get_variable(graph, session, 'item_factors')\n",
    "    bi = get_variable(graph, session, 'item_bias').reshape(-1)\n",
    "\n",
    "    pred_all = U[uid_val].dot(I.T) + bi\n",
    "    top_val = (-pred_all).argsort(axis=1)[:, :5]\n",
    "\n",
    "    imp_baseline = baseline.copy()\n",
    "    imp_baseline[known_mask] = top_val\n",
    "\n",
    "    return precision(val_indptr, val_items, imp_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: precision: 0.064\n",
      "\n",
      "\n",
      "epoch 02: precision: 0.087\n",
      "\n",
      "epoch 03: precision: 0.110\n",
      "\n",
      "epoch 04: precision: 0.125\n",
      "\n",
      "epoch 05: precision: 0.138\n",
      "\n",
      "epoch 06: precision: 0.147\n",
      "\n",
      "epoch 07: precision: 0.150\n",
      "\n",
      "epoch 08: precision: 0.153\n",
      "\n",
      "epoch 09: precision: 0.154\n",
      "\n",
      "epoch 10: precision: 0.151\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    train_idx_shuffle = np.arange(uid.shape[0])\n",
    "    np.random.shuffle(train_idx_shuffle)\n",
    "    batches = prepare_batches(train_idx_shuffle, 5000)\n",
    "\n",
    "    progress = tqdm(total=len(batches))\n",
    "    for idx in batches:\n",
    "        pos_samples = len(idx)\n",
    "        neg_samples = pos_samples * K \n",
    "\n",
    "        label = np.concatenate([\n",
    "                    np.ones(pos_samples, dtype='float32'), \n",
    "                    np.zeros(neg_samples, dtype='float32')\n",
    "                ]).reshape(-1, 1)\n",
    "\n",
    "        neg_users = np.random.randint(low=0, high=num_users, \n",
    "                                      size=neg_samples, dtype='int32')\n",
    "        neg_items = np.random.randint(low=0, high=num_items,\n",
    "                                      size=neg_samples, dtype='int32')\n",
    "\n",
    "        batch_uid = np.concatenate([uid[idx], neg_users]).reshape(-1, 1)\n",
    "        batch_iid = np.concatenate([iid[idx], neg_items]).reshape(-1, 1)\n",
    "\n",
    "        feed_dict = {\n",
    "            place_user: batch_uid,\n",
    "            place_item: batch_iid,\n",
    "            place_y: label,\n",
    "        }\n",
    "        _, l = session.run([step, loss], feed_dict)\n",
    "        \n",
    "        progress.update(1)\n",
    "        progress.set_description('%.3f' % l)\n",
    "    progress.close()\n",
    "\n",
    "    val_precision = calculate_validation_precision(graph, session, uid_val)\n",
    "    print('epoch %02d: precision: %.3f' % (i+1, val_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Personalized Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_factors = 128\n",
    "lambda_user = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "lambda_bias = 0.0000001\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_variable(size, dim, name=None):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n",
    "\n",
    "def embed(inputs, size, dim, name=None):\n",
    "    emb = init_variable(size, dim, name)\n",
    "    return tf.nn.embedding_lookup(emb, inputs)\n",
    "\n",
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_user = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_item_pos = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_item_neg = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "\n",
    "    user_factors = embed(place_user, num_users, num_factors, \"user_factors\")\n",
    "    item_factors = init_variable(num_items, num_factors, \"item_factors\")\n",
    "    item_factors_pos = tf.nn.embedding_lookup(item_factors, place_item_pos)\n",
    "    item_factors_neg = tf.nn.embedding_lookup(item_factors, place_item_neg)\n",
    "\n",
    "    item_bias = init_variable(num_items, 1, \"item_bias\")\n",
    "    item_bias_pos = tf.nn.embedding_lookup(item_bias, place_item_pos)\n",
    "    item_bias_pos = tf.reshape(item_bias_pos, [-1, 1])\n",
    "    item_bias_neg = tf.nn.embedding_lookup(item_bias, place_item_neg)\n",
    "    item_bias_neg = tf.reshape(item_bias_neg, [-1, 1])\n",
    "\n",
    "    pred_pos = item_bias_pos + tf.reduce_sum(user_factors * item_factors_pos, axis=2)\n",
    "    pred_neg = item_bias_neg + tf.reduce_sum(user_factors * item_factors_neg, axis=2)\n",
    "\n",
    "    pred_diff = pred_pos - pred_neg\n",
    "\n",
    "    loss_bpr = - tf.reduce_mean(tf.log(tf.sigmoid(pred_diff)))\n",
    "    loss_reg = lambda_user * tf.reduce_sum(user_factors * user_factors) + \\\n",
    "          lambda_item * tf.reduce_sum(item_factors_pos * item_factors_pos) + \\\n",
    "          lambda_item * tf.reduce_sum(item_factors_neg * item_factors_neg) + \\\n",
    "          lambda_bias * tf.reduce_sum(item_bias_pos) + \\\n",
    "          lambda_bias * tf.reduce_sum(item_bias_neg)\n",
    "        \n",
    "    loss_total = loss_bpr + loss_reg\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss_total)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "size_total = uid.shape[0]\n",
    "size_sample = 15000\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(100):\n",
    "    for k in range(30):\n",
    "        idx = np.random.randint(low=0, high=size_total, size=size_sample)\n",
    "\n",
    "        batch_uid = uid[idx].reshape(-1, 1)\n",
    "        batch_iid_pos = iid[idx].reshape(-1, 1)\n",
    "        batch_iid_neg = np.random.randint(\n",
    "            low=0, high=num_items, size=(size_sample, 1), dtype='int32')\n",
    "\n",
    "        feed_dict = {\n",
    "            place_user: batch_uid,\n",
    "            place_item_pos: batch_iid_pos,\n",
    "            place_item_neg: batch_iid_neg,\n",
    "        }\n",
    "        _, l = session.run([step, loss_bpr], feed_dict)\n",
    "\n",
    "    val_precision = calculate_validation_precision(graph, session, uid_val)\n",
    "    print('epoch %02d: recall: %.3f' % (i+1, val_precision))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
